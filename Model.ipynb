{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10331, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('reddit_comments.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10331 entries, 0 to 10330\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   author   10331 non-null  object\n",
      " 1   comment  10331 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 161.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishnu/Desktop/Text-Completion/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /Users/vishnu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|startoftext|> my comment will be buried but what he heck i was probably 8 or 9 at the time, trampolines were the best shit in the world. <|endoftext|>', '<|startoftext|> one day doing flips i lost my sense of reality or something and went head first strait into the ground with my eyes squeezing tight knowing my fate is at hand...i open my eyes and my head is inches above the ground and im dangling there. <|endoftext|>', '<|startoftext|> my legs caught the space between the springs luckily, and i knew i had nothing to do with it. <|endoftext|>', '<|startoftext|> i wasnt trying to save myself but i believe God was watchin out for me. <|endoftext|>', '<|startoftext|> thanks Jesus, I love you reddit dont downvote me lol TLDR flying spaghetti monster lassowed me inches from certain death! <|endoftext|>']\n",
      "Train Test Split is Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [00:00<00:00, 1542.96 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 2816.05 examples/s]\n",
      "/Users/vishnu/Desktop/Text-Completion/.venv/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [339/339 08:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.783200</td>\n",
       "      <td>1.645998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.518900</td>\n",
       "      <td>1.714984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.195200</td>\n",
       "      <td>1.891808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 1.891808271408081, 'eval_runtime': 2.552, 'eval_samples_per_second': 39.184, 'eval_steps_per_second': 2.743, 'epoch': 3.0}\n",
      "Model training complete and saved!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download NLTK tokenizer\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Load dataset from CSV\n",
    "df = pd.read_csv(\"cleaned_reddit_comments.csv\")  \n",
    "\n",
    "# Function to split comments into sentences\n",
    "def preprocess_comments(comments):\n",
    "    sentences = []\n",
    "    for comment in comments:\n",
    "        for sentence in sent_tokenize(comment):\n",
    "            sentences.append(f\"<|startoftext|> {sentence} <|endoftext|>\") \n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Preprocess the dataset (sentence-level)\n",
    "sentences = preprocess_comments(df['comment'].dropna().tolist())\n",
    "sentences = sentences[:1000]\n",
    "print(sentences[:5])\n",
    "# Train-validation split (90% train, 10% validation)\n",
    "train_texts, val_texts = train_test_split(sentences, test_size=0.1, random_state=42)\n",
    "print(\"Train Test Split is Completed\")\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Ensure tokenizer has padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create dataset dictionary\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\"text\": train_texts}),\n",
    "    \"validation\": Dataset.from_dict({\"text\": val_texts}),\n",
    "})\n",
    "\n",
    "# Fix: Add labels for loss calculation\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=50)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()  # Add labels\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=1000,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",  \n",
    "    save_strategy=\"epoch\",      \n",
    "    learning_rate=0.001,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset, \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"./gpt2_reddit_model\")\n",
    "tokenizer.save_pretrained(\"./gpt2_reddit_model\")\n",
    "\n",
    "print(\"Model training complete and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
